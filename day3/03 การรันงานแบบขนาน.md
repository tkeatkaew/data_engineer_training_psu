### üöÄ **‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà 3: ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô‡πÅ‡∏ö‡∏ö‡∏Ç‡∏ô‡∏≤‡∏ô (Parallel Job Execution) ‡∏ö‡∏ô LANTA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏°‡∏±‡∏ò‡∏¢‡∏°‡∏®‡∏∂‡∏Å‡∏©‡∏≤** üöÄ  

---

## **üîπ ‡∏ö‡∏ó‡∏ô‡∏≥: ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ LANTA?**  
üí° ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏°‡∏µ **1-8 CPU Cores** ‡πÅ‡∏•‡∏∞ GPU ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏à‡∏≥‡∏Å‡∏±‡∏î  
üí° **LANTA** ‡πÄ‡∏õ‡πá‡∏ô‡∏ã‡∏π‡πÄ‡∏õ‡∏≠‡∏£‡πå‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ **31,744 Cores ‡πÅ‡∏•‡∏∞ 704 NVIDIA A100 GPUs** üöÄ  
üí° **LANTA ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô 30-100 ‡πÄ‡∏ó‡πà‡∏≤!**  

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô **AI Training** ‡πÅ‡∏•‡∏∞ **‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏•‡∏Å‡∏∏‡∏•**  

| ‡∏£‡∏∞‡∏ö‡∏ö | CPU | GPU | ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô ResNet50 AI Model |
|---|---|---|---|
| Laptop ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ | Intel i7-9750H (6 Cores) | GTX 1650 | 3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |
| LANTA (1 CPU Node) | AMD EPYC 7713 (128 Cores) | ‡πÑ‡∏°‡πà‡∏°‡∏µ | 30 ‡∏ô‡∏≤‡∏ó‡∏µ |
| LANTA (1 GPU Node) | AMD EPYC 7713 (64 Cores) | 4x NVIDIA A100 | **5 ‡∏ô‡∏≤‡∏ó‡∏µ!** |

**üìå ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ô‡∏µ‡πâ ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ**  
‚úÖ **‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏ö‡∏ö‡∏Ç‡∏ô‡∏≤‡∏ô (MPI) ‡∏î‡πâ‡∏ß‡∏¢ CPU**  
‚úÖ **‡πÉ‡∏ä‡πâ GPU ‡∏Ç‡∏≠‡∏á LANTA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Training**  
‚úÖ **‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á LANTA ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ**  

---

## **üîπ 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**
‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô **MPI ‡πÅ‡∏•‡∏∞ GPU Acceleration**  
```bash
module load cray-mpich/8.1.27 gcc/12.2.0 cudatoolkit/23.3_12.0
```
‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏ã‡∏≠‡∏ü‡∏ï‡πå‡πÅ‡∏ß‡∏£‡πå‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡πâ‡∏≤‡∏á  
```bash
module avail
```
*(‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô LANTA)*  

---
## **üîπ 2. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà 1: ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î FLOPS ‡∏î‡πâ‡∏ß‡∏¢ MPI ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Core ‡∏´‡∏£‡∏∑‡∏≠ Process
**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡πÉ‡∏ä‡πâ MPI ‡πÄ‡∏û‡∏∑‡πà‡∏≠ benchmark FLOPS ‡∏Ç‡∏≠‡∏á cpu ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ core ‡∏´‡∏£‡∏∑‡∏≠ process

üìå **‡πÇ‡∏Ñ‡πâ‡∏î C: `benchmark.c`** 
```c
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

#define FLOPS_PER_ITER 2  // 1 multiplication + 1 addition
#define TARGET_TOTAL_FLOPS 3600000000000ULL  // 3.6 TFLOPs

int main(int argc, char* argv[]) {
    int rank, size;
    MPI_Init(&argc, &argv);

    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    // Compute per-process workload
    unsigned long long flops_per_rank = TARGET_TOTAL_FLOPS / size;
    unsigned long long N = flops_per_rank / FLOPS_PER_ITER;

    // Dummy variables for FMA loop
    double a = 1.0000001, b = 1.0000002, c = 0.0;
    double result = 0.0;

    MPI_Barrier(MPI_COMM_WORLD);
    double start_time = MPI_Wtime();

    // Heavy floating point loop
    for (unsigned long long i = 0; i < N; i++) {
        c += a * b;
    }

    MPI_Barrier(MPI_COMM_WORLD);
    double end_time = MPI_Wtime();
    double elapsed = end_time - start_time;

    // Local GFLOPS
    double local_flops = N * FLOPS_PER_ITER;
    double local_gflops = local_flops / elapsed / 1e9;

    printf("Rank %d: Time = %.6f s, GFLOPS = %.2f\n", rank, elapsed, local_gflops);

    // Collect overall performance
    double total_flops, max_time;
    MPI_Reduce(&local_flops, &total_flops, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    MPI_Reduce(&elapsed, &max_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        double total_gflops = total_flops / max_time / 1e9;
        printf("\n==== MPI FLOPS Benchmark Summary ====\n");
        printf("Processes        : %d\n", size);
        printf("Total workload   : %.2f TFLOPs\n", total_flops / 1e12);
        printf("Elapsed time     : %.6f s (max across ranks)\n", max_time);
        printf("Achieved GFLOPS  : %.2f GFLOPS\n", total_gflops);
        printf("Efficiency vs 1.0 TFLOPS theoretical: %.1f%%\n", (total_gflops / 1000.0) * 100);
        printf("=====================================\n");
    }

    MPI_Finalize();
    return 0;
}
```

üìå **‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏Ñ‡πâ‡∏î C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ MPI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Benchmark**  
```bash
mpicc -O3 -march=native -ffast-math -o benchmark benchmark.c
```

üìå **‡∏Ç‡∏≠‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡πÅ‡∏ö‡∏ö‡∏≠‡∏¥‡∏ô‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏≠‡∏Ñ‡∏ó‡∏µ‡∏ü**  
```bash
salloc --nodes=1 --ntasks=128 --partition=compute-devel --time=00:10:00 -A cb900905
```

üìå **‡∏™‡∏±‡πà‡∏á‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠ Benchmark**  
```bash
srun benchmark
```

## **üîπ 3. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà 2: ‡∏£‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÅ‡∏ö‡∏ö‡∏Ç‡∏ô‡∏≤‡∏ô (MPI) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å**  
**‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:** ‡πÉ‡∏ä‡πâ MPI ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÜ CPU ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì  
```bash
module use /project/cb900907-hpctgn/modules
module load nano
module load tree
```

üìå **‡πÇ‡∏Ñ‡πâ‡∏î C: `mpi_pi.c`**  
```c
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>

int main(int argc, char** argv) {
    int rank, size, i, n = 100000000;
    double sum = 0.0, pi, x, step = 1.0 / n;

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    for (i = rank; i < n; i += size) {
        x = (i + 0.5) * step;
        sum += 4.0 / (1.0 + x * x);
    }

    MPI_Reduce(&sum, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);
    if (rank == 0) {
        pi *= step;
        printf("Pi ‚âà %.16f\n", pi);
    }

    MPI_Finalize();
    return 0;
}
```
üìå **‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏Ñ‡πâ‡∏î C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ MPI**  
```bash
module load gcc/12.2.0 cray-mpich/8.1.27
```
``` bash
mpicc mpi_pi.c -o mpi_pi
```
üìå **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô: `mpi_pi_job.sh`**  
```bash
#!/bin/bash
#SBATCH --job-name=mpi_pi_test
#SBATCH --output=mpi_pi_output.txt
#SBATCH --error=mpi_pi_error.txt
#SBATCH --time=00:05:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=128
#SBATCH --partition=compute
#SBATCH -A cb900908

module load cray-mpich/8.1.27 gcc/12.2.0

srun ./mpi_pi
```
üìå **‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤ Slurm Queue**  
```bash
sbatch mpi_pi_job.sh
```
üìå **‡πÄ‡∏ä‡πá‡∏Ñ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏á‡∏≤‡∏ô**
```bash
myqueue
```

üìå **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**  
```bash
cat mpi_pi_output.txt
```
**üìå ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏ö‡∏ô Laptop ‡∏Å‡∏±‡∏ö LANTA**
| ‡∏£‡∏∞‡∏ö‡∏ö | CPU | ‡πÄ‡∏ß‡∏•‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ Pi (n = 100 ‡∏•‡πâ‡∏≤‡∏ô) |
|---|---|---|
| Laptop ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ | Intel i7 (6 Cores) | 10 ‡∏ô‡∏≤‡∏ó‡∏µ |
| LANTA (128 Cores) | AMD EPYC 7713 | **10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ!** | 

---

## **üîπ 4. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà 3: ‡πÉ‡∏ä‡πâ GPU ‡∏Ç‡∏≠‡∏á LANTA ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö AI Training**  
üìå **‡πÇ‡∏Ñ‡πâ‡∏î Python: `train_ai.py`**  
```python
import tensorflow as tf
from tensorflow import keras

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ GPU ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
print("GPU Available:", tf.config.list_physical_devices('GPU'))

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ResNet50
model = keras.applications.ResNet50(weights=None, input_shape=(224, 224, 3), classes=10)
model.compile(optimizer='adam', loss='categorical_crossentropy')

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏•‡∏≠‡∏á
import numpy as np
x_train = np.random.rand(1000, 224, 224, 3)
y_train = np.random.randint(10, size=(1000,))

# Train 10 epochs
model.fit(x_train, keras.utils.to_categorical(y_train, 10), epochs=10)
```
üìå **‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô: `ml_train_job.sh`**  
```bash
#!/bin/bash
#SBATCH --job-name=ml_training
#SBATCH --output=ml_output.txt
#SBATCH --error=ml_error.txt
#SBATCH --time=02:00:00
#SBATCH --nodes=1
#SBATCH --gres=gpu:4
#SBATCH --partition=gpu
#SBATCH -A cb900908

module load cudatoolkit/23.3_12.0 tensorflow/2.5

python train_ai.py
```
üìå **‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô AI Training**  
```bash
sbatch ml_train_job.sh
```
üìå **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå**  
```bash
cat ml_output.txt
```

**üìå ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏≠‡∏á AI Training ‡∏ö‡∏ô Laptop vs. LANTA**
| ‡∏£‡∏∞‡∏ö‡∏ö | CPU | GPU | ‡πÄ‡∏ß‡∏•‡∏≤ Training ResNet50 (10 Epochs) |
|---|---|---|---|
| Laptop | Intel i7-9750H | GTX 1650 | 3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á |
| LANTA (1 GPU Node) | AMD EPYC 7713 (64 Cores) | 4x NVIDIA A100 | **5 ‡∏ô‡∏≤‡∏ó‡∏µ!** |

üöÄ *‡πÉ‡∏ä‡πâ LANTA ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ 36 ‡πÄ‡∏ó‡πà‡∏≤!* üöÄ  

---

## **üîπ 5. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Job Performance Analysis)**
‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á:  
```bash
sacct -j <JobID> --format=JobID,Elapsed,MaxRSS,MaxVMSize
```
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:
```
JobID    Elapsed    MaxRSS  MaxVMSize
12345    00:05:32   5GB     10GB
```
üí° **‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î**  

---

## **üîπ 6. ‡∏™‡∏£‡∏∏‡∏õ**
‚úÖ **LANTA ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ Laptop ‡∏´‡∏•‡∏≤‡∏¢‡∏™‡∏¥‡∏ö‡πÄ‡∏ó‡πà‡∏≤!**  
‚úÖ **‡πÉ‡∏ä‡πâ MPI ‡πÅ‡∏•‡∏∞ GPU acceleration ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì**  
‚úÖ **‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ `sbatch` ‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ `squeue` ‡πÅ‡∏•‡∏∞ `sacct`**  

üìå *‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ó‡∏µ‡∏° HPC Ignite ‡∏ó‡∏µ‡πà*  
**üìß hpc-ignite@thaisc.io**  

---

